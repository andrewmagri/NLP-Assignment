Index: Models/LSTM.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># https://www.datatechnotes.com/2018/12/rnn-example-with-keras-simplernn-in.html\r\nimport datetime\r\n\r\nfrom keras.layers import LSTM, Dense, Dropout, Embedding, Activation, Bidirectional\r\nfrom keras.models import Sequential\r\nfrom sklearn.model_selection import train_test_split\r\nfrom tensorflow.python.keras.utils.vis_utils import plot_model\r\nfrom tensorboard.plugins.hparams import api as hp\r\nfrom DataRetrival import *\r\n\r\nimport tensorflow as tf\r\n\r\nfrom WordEmbeddings import extract_tfidf_featuriser\r\n\r\nN = 1000\r\nTp = 800\r\nepochs = 1000\r\nnoOfNeuronsInOutputLayer = 7\r\ndateAndTimeNow = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n\r\n# Obtaining the csv file having all outputs on 1 coloumn\r\n# X49LagAccData,y49lagAccData = returnXYData(49, 5)\r\n\r\n# X49LagAccData.to_pickle(\"X49LagAccData\")\r\n# y49lagAccData.to_pickle(\"y49lagAccData\")\r\n\r\ntrainTextDir = \"..\\\\Semeval2018-Task2-EmojiPrediction\\\\Data\\\\tweet_by_ID_04_2_2021__05_27_42.txt.text\"\r\ntrainLabelDir = \"..\\\\Semeval2018-Task2-EmojiPrediction\\\\Data\\\\tweet_by_ID_04_2_2021__05_27_42.txt.labels\"\r\ntestTextDir = \"..\\\\Semeval2018-Task2-EmojiPrediction\\\\test\\\\us_test.text\"\r\ntestLabelDir = \"..\\\\Semeval2018-Task2-EmojiPrediction\\\\test\\\\us_test.labels\"\r\ntrain_tweets = get_train_data(trainTextDir, trainLabelDir)\r\ntest_tweets = get_test_data(testTextDir, testLabelDir)\r\n\r\nX_train = train_tweets.tweetsText[:100]\r\ny_train = train_tweets.tweetsLabel[:100]\r\n\r\n\r\nX_test = test_tweets.tweetsText\r\ny_test = test_tweets.tweetsLabel\r\n\r\n\r\ntfidf_featuriser = extract_tfidf_featuriser(train_tweets.tweetsText[:100])\r\nX_train = tfidf_featuriser.transform(train_tweets.tweetsText[:100])\r\nX_test = tfidf_featuriser.transform(test_tweets.tweetsText[:100])\r\n\r\nHP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([7]))\r\nHP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([32]))\r\nHP_ACT_FUNC = hp.HParam('activation_function', hp.Discrete([\"softmax\"]))\r\nHP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete([\"rmsprop\"]))\r\n\r\nMETRIC_ACCURACY = 'accuracy'\r\n\r\nwith tf.summary.create_file_writer('logs/hparam_tuning/' + dateAndTimeNow).as_default():\r\n    hp.hparams_config(\r\n        hparams=[HP_NUM_UNITS, HP_BATCH_SIZE, HP_ACT_FUNC],\r\n        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\r\n    )\r\n\r\n\r\ndef lstm(hparams, input_dimension, output_dimension=300, max_length):\r\n    step = X_train.shape[1]\r\n    model = Sequential()\r\n    model.add(Embedding(input_dimension,\r\n                        output_dimension,\r\n                        input_length=max_length))\r\n    model.add(Dropout(0.25))\r\n    model.add(Bidirectional(LSTM(input_shape=(step, 1), units=10, activation=\"softmax\")))\r\n    model.add(Bidirectional(LSTM(input_shape=(step, 1), units=10, activation=\"softmax\")))\r\n    model.add(Dense(20, activation=\"softmax\"))\r\n    model.add(Activation('softmax'))\r\n    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['acc'])\r\n    model.summary()\r\n    plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\r\n\r\n    model.fit(X_train, y_train, epochs=epochs, batch_size=hparams[HP_BATCH_SIZE])  # Run with 1 epoch to speed things up for demo purposes\r\n    test_predictions = model.predict(X_test)\r\n    _, accuracy = model.evaluate(X_test, y_test)\r\n\r\n    return accuracy\r\n\r\n\r\ndef run(run_dir, hparams):\r\n    with tf.summary.create_file_writer(run_dir).as_default():\r\n        hp.hparams(hparams)  # record the values used in this trial\r\n        accuracy = train_test_model2(hparams)\r\n        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\r\n\r\n\r\nsession_num = 0\r\n\r\nfor num_units in HP_NUM_UNITS.domain.values:\r\n    for batch_size in HP_BATCH_SIZE.domain.values:\r\n        for act_func in HP_ACT_FUNC.domain.values:\r\n            for optimizer in HP_OPTIMIZER.domain.values:\r\n                hparams = {\r\n                    HP_NUM_UNITS: num_units,\r\n                    HP_BATCH_SIZE: batch_size,\r\n                    HP_ACT_FUNC: act_func,\r\n                    HP_OPTIMIZER: optimizer,\r\n                }\r\n                run_name = \"run-%d\" % session_num\r\n                print('--- Starting trial: %s' % run_name)\r\n                print({h.name: hparams[h] for h in hparams})\r\n                run('logs/hparam_tuning/' + dateAndTimeNow + run_name, hparams)\r\n                session_num += 1\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Models/LSTM.py b/Models/LSTM.py
--- a/Models/LSTM.py	(revision 48cd4f8c04fbe856124e8ddeedadde446905aac9)
+++ b/Models/LSTM.py	(date 1613914433300)
@@ -82,7 +82,7 @@
 def run(run_dir, hparams):
     with tf.summary.create_file_writer(run_dir).as_default():
         hp.hparams(hparams)  # record the values used in this trial
-        accuracy = train_test_model2(hparams)
+        accuracy = lstm(hparams)
         tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)
 
 
